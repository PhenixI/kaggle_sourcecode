{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os,random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input,Dropout,Flatten,Convolution2D,MaxPooling2D,Dense,Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint,Callback,EarlyStopping\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Preparing the Data\n",
    "\n",
    "1. resize the images to 64x64\n",
    "2. sample 2000 images(8%) of the data to run efficiently\n",
    "3. separate cats and dogs for exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 of 25000\n",
      "Processed 1000 of 25000\n",
      "Processed 2000 of 25000\n",
      "Processed 3000 of 25000\n",
      "Processed 4000 of 25000\n",
      "Processed 5000 of 25000\n",
      "Processed 6000 of 25000\n",
      "Processed 7000 of 25000\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = 'F:/Data Science/Dataset/DogVsCat/train/'\n",
    "TEST_DIR = 'F:/Data Science/Dataset/DogVsCat/test/'\n",
    "MODEL_WEIGHT_DIR = 'F:/Data Science/Kaggle/kaggle_sourcecode/DogsvsCats/'\n",
    "\n",
    "ROWS = 64\n",
    "COLS = 64\n",
    "CHANNELS = 3\n",
    "\n",
    "train_images = [TRAIN_DIR + i for i in os.listdir(TRAIN_DIR)]#use this for full datase\n",
    "train_dogs = [TRAIN_DIR + i for i in os.listdir(TRAIN_DIR) if 'dog' in i]\n",
    "train_cats = [TRAIN_DIR + i for i in os.listdir(TRAIN_DIR) if 'cat' in i]\n",
    "\n",
    "test_images = [TEST_DIR + i for i in os.listdir(TEST_DIR)]\n",
    "\n",
    "#slice datasets for memory efficiency , delete if using full dataset\n",
    "#train_images = train_dogs[:2000] + train_cats[:2000]\n",
    "#train_images = train_dogs[:1]\n",
    "random.shuffle(train_images)\n",
    "#test_images = test_images[:25]\n",
    "\n",
    "def read_image(file_path):\n",
    "    img = Image.open(file_path)\n",
    "    img_res = img.resize((ROWS,COLS),resample = PIL.Image.BICUBIC)\n",
    "    return img_res\n",
    "\n",
    "def prep_data(images):\n",
    "    count = len(images)\n",
    "    data = np.ndarray((count,CHANNELS,ROWS,COLS),dtype = np.uint8)\n",
    "    \n",
    "    for i, image_file in enumerate(images):\n",
    "        image = read_image(image_file)   \n",
    "        img_np = np.asarray(image).astype(np.uint8)\n",
    "        #print (img_np)\n",
    "        data[i] = img_np.T\n",
    "        #print (img_np.T)\n",
    "        #print (i)\n",
    "        if i%1000 == 0:\n",
    "            print('Processed {} of {}'.format(i,count))\n",
    "    return data\n",
    "\n",
    "train=prep_data(train_images)\n",
    "test=prep_data(test_images)\n",
    "\n",
    "print(\"Train shape: {}\".format(train.shape))\n",
    "print(\"Test shape : {}\".format(test.shape))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the Labels\n",
    "\n",
    "We're dealing with a binary classification problem here - (1) dog (0) cat.\n",
    "The lables is obtained by looping over the file names in the train directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in train_images:\n",
    "    if 'dog' in i:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "\n",
    "sns.countplot(labels)\n",
    "sns.plt.title('Cats and Dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out Cats and Dogs\n",
    "\n",
    "A quick side-by-side comparison of the animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_cats_and_dogs(idx):\n",
    "    cat = read_image(train_cats[idx])\n",
    "    dog = read_image(train_dogs[idx])\n",
    "    \n",
    "    plt.figure(figsize = (8,4))\n",
    "    plt.subplot(121)\n",
    "    plt.title('Cat')\n",
    "    plt.imshow(cat)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.title('Dog')\n",
    "    plt.imshow(dog)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for idx in range(0,5):\n",
    "    show_cats_and_dogs(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Cat and Dog Photo\n",
    "\n",
    "for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dog_avg = np.array([dog[0].T for i,dog in enumerate(train) if labels[i] == 1]).mean(axis = 0)\n",
    "plt.imshow(dog_avg)\n",
    "plt.title('Your Average Dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_avg = np.array([cat[0].T for i ,cat in enumerate(train) if labels[i] == 0]).mean(axis = 0)\n",
    "plt.imshow(cat_avg)\n",
    "plt.title('Your Average Cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatDogNet-16\n",
    "*  based on VGG-16: very deep convolutional Networks for Large-Scale Visual Recognition\n",
    "\n",
    "A scaled down version of the VGG-16,with a few notable changes\n",
    "1. Number of convolution filters cut in half, (64->32),(128->64),(256->128),(512->256)\n",
    "2. fully connected(Dense) layers scaled down (FC-4096,FC-4096,FC-1000)->(FC-256,FC-256,FC-1)\n",
    "3. Optimizer changed to RMSprop\n",
    "4. Output layer activation set to sigmoid for binary crossentropy\n",
    "5. Some Layes commemted out for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr = 1e-4)\n",
    "objective = 'binary_crossentropy'\n",
    "\n",
    "def catdog():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Convolution2D(32,3,3,border_mode = 'same',input_shape = (3,ROWS,COLS),activation = 'relu'))\n",
    "    model.add(Convolution2D(32,3,3,border_mode = 'same',activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2),dim_ordering = 'th'))\n",
    "    \n",
    "    model.add(Convolution2D(64,3,3,border_mode = 'same',activation = 'relu'))\n",
    "    model.add(Convolution2D(64,3,3,border_mode = 'same',activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),dim_ordering = 'th'))\n",
    "    \n",
    "    model.add(Convolution2D(128,3,3,border_mode = 'same',activation = 'relu'))\n",
    "    model.add(Convolution2D(128,3,3,border_mode = 'same',activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2),dim_ordering = 'th'))\n",
    "    \n",
    "    model.add(Convolution2D(256,3,3,border_mode = 'same',activation = 'relu'))\n",
    "    model.add(Convolution2D(256,3,3,border_mode = 'same',activation = 'relu'))\n",
    "    #model.add(Convolution2D(256,3,3,border_model = 'same',activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2),dim_ordering = 'th'))\n",
    "    \n",
    "    # model.add(Convolution2D(256, 3, 3, border_mode='same', activation='relu'))\n",
    "    # model.add(Convolution2D(256, 3, 3, border_mode='same', activation='relu'))\n",
    "    # model.add(Convolution2D(256, 3, 3, border_mode='same', activation='relu'))\n",
    "    # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256,activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(256,activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile(loss = objective,optimizer = optimizer,metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = catdog()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Predict\n",
    "\n",
    "1. use Keras's early stopping callback to end training when the validation loss stop improving, otherwise the model will overfit\n",
    "2. tracking the loss history on each epoch to visualize the overfitting trend\n",
    "3. A slice of 1000 images was used to fit the model for CPU efficency. The model's performance improves significantly when used on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_epoch = 10\n",
    "batch_size = 16\n",
    "\n",
    "##Callback for loss logging per epoch\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self,logs = {}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    def on_epoch_end(self,batch,logs = {}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        \n",
    "early_stopping = EarlyStopping(monitor = 'val_loss',patience = 3,verbose = 1,mode = 'auto')\n",
    "\n",
    "def run_catdog():\n",
    "    history = LossHistory()\n",
    "    model.fit(train,labels,batch_size = batch_size,nb_epoch = nb_epoch,\n",
    "              validation_split = 0.25,verbose = 0,shuffle = True,callbacks = [history,early_stopping])\n",
    "    predictions = model.predict(test,verbose = 0)\n",
    "    return predictions,history\n",
    "\n",
    "predictions,history = run_catdog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = history.losses\n",
    "val_loss = history.val_losses\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('VGG-16 Loss Trend')\n",
    "plt.plot(loss,'blue',label = 'Training Loss')\n",
    "plt.plot(val_loss,'green',label = 'Validation Loss')\n",
    "plt.xticks(range(0,nb_epoch)[0::2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. what's the confident is the model\n",
    "2. Run on the full dataset with a GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    if predictions[i,0]>=0.5:\n",
    "        print('I am {:.2%} sure this is a Dog'.format(predictions[i][0]))\n",
    "    else:\n",
    "        print('I am {:.2%}sure this is a Cat'.format(1-predictions[i][0]))\n",
    "    \n",
    "    plt.imshow(test[i].T)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "json_string = model.to_json()\n",
    "\n",
    "import json\n",
    "models_name = 'VGG_16_loss_trend.txt'\n",
    "model_path = os.path.join(MODEL_WEIGHT_DIR,models_name)\n",
    "with open(model_path,'w') as outfile:\n",
    "    json.dump(json_string,outfile)\n",
    "#model =  model_from_json(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_name = 'VGG_16_loss_trend_full_weights.txt'\n",
    "weight_path = os.path.join(MODEL_WEIGHT_DIR,weights_name)\n",
    "model.save_weights(weight_path)\n",
    "\n",
    "#model.load_weights(weight_path,by_name = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
